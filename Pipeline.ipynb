{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to load Datasets\n",
    "- NLI Dataset\n",
    "- NMT Dataset\n",
    "- Tree Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "from collections import OrderedDict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = \"./gensen/data/corpora/allnli.train.txt.clean.noblank\"\n",
    "EN_FILE = \"./english.tok\"\n",
    "DE_FILE = \"./german.tok\"\n",
    "\n",
    "#Const. Parsing FILES\n",
    "CPT_X_FILE = \"./tree_data/en.txt.tok.out\"\n",
    "CPT_Y_FILE  = \"./tree_data/pt.out\"\n",
    "\n",
    "#Vocab Files\n",
    "COMMON_VOCAB_FILE = \"./words.txt\"\n",
    "GERMAN_VOCAB_FILE = \"./german_words.txt\"\n",
    "TREE_VOCAB_FILE = \"./tree_words.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vocabs\n",
    "vocab = tf.contrib.lookup.index_table_from_file(COMMON_VOCAB_FILE, num_oov_buckets=1)\n",
    "vocab_german = tf.contrib.lookup.index_table_from_file(GERMAN_VOCAB_FILE,num_oov_buckets=1)\n",
    "vocab_tree  = tf.contrib.lookup.index_table_from_file(TREE_VOCAB_FILE,num_oov_buckets=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=30, shape=(3,), dtype=int64, numpy=array([7844,    7,  197])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vocab.lookup(tf.constant([\"hello\",\"a\",\"great\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_data(dataset,n_rows=4):\n",
    "    \"\"\"Function to view any dataset upto first n rows\"\"\"\n",
    "    itr = dataset.make_one_shot_iterator()\n",
    "    for i in range(n_rows):\n",
    "            next_item = itr.get_next()\n",
    "            print(next_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToTokens(dataset,vocab):\n",
    "    \"\"\"Converts sentences into tokens, maps them to ints, computes each sentence len\"\"\"\n",
    "    dataset = dataset.map(lambda sentence:tf.string_split([sentence]).values)\n",
    "    dataset = dataset.map(lambda token: { 'sentence': vocab.lookup(token), 'len': tf.size(token) } )    \n",
    "    #dataset = dataset.map(lambda x:someFn(x))\n",
    "    return dataset\n",
    "\n",
    "#Loads data from text file\n",
    "def load_dataset(filepath,vocab):\n",
    "    \n",
    "    dataset = tf.data.TextLineDataset(filepath)\n",
    "    \n",
    "    HypothesisData = dataset.map(lambda sentence: tf.string_split([sentence],\"\\t\").values[0])\n",
    "    PremisesData = dataset.map(lambda sentence: tf.string_split([sentence],\"\\t\").values[1])\n",
    "    LabelsData = dataset.map(lambda sentence: tf.string_split([sentence],\"\\t\").values[2])\n",
    "    \n",
    "    \n",
    "    HypothesisData = convertToTokens(HypothesisData,vocab)\n",
    "    PremisesData = convertToTokens(PremisesData,vocab)\n",
    "    \n",
    "   \n",
    "    #LabelsData = convertToLabels(LabelsData)\n",
    "    return (HypothesisData,PremisesData,LabelsData)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLabelNumpy(L):\n",
    "    itrH = L.make_one_shot_iterator()\n",
    "\n",
    "    l = []\n",
    "    mp = { 'neutral':0,\n",
    "           'contradiction':1,\n",
    "           'entailment':2,\n",
    "            '-':3\n",
    "         }\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            next_item = itrH.get_next().numpy().decode('utf-8')\n",
    "            l.append(mp[next_item])\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    return np.array(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_NLI_dataset(file,batch_size=32,buffer_size=1024,prefetch_size=5):\n",
    "    \"\"\"Accepts NLI Train File Creates Datasets of Hypothesis, Premises and Labels\"\"\"\n",
    "    \"\"\"Returns Labels as One Hot Vectors\"\"\"\n",
    "    \n",
    "    global vocab\n",
    "\n",
    "    H,P,L = load_dataset(file,vocab)\n",
    "    label_numpy = createLabelNumpy(L)\n",
    "    labelDataset = tf.data.Dataset.from_tensor_slices(label_numpy)\n",
    "    depth = 4\n",
    "    labelDataset = labelDataset.map(lambda x:tf.one_hot(x,4))\n",
    "    \n",
    "    HD = H\n",
    "    PD = P\n",
    "    LD = labelDataset\n",
    "    \n",
    "    dataset = tf.data.Dataset.zip((HD,PD,LD))\n",
    "    dataset = (dataset\n",
    "              .shuffle(buffer_size=buffer_size)\n",
    "               .padded_batch(batch_size=batch_size,padded_shapes = ({'sentence':[None],'len':[]},\n",
    "                                                                   {'sentence':[None],'len':[]},\n",
    "                                                                    [None]))\n",
    "               .prefetch(prefetch_size)\n",
    "              )\n",
    "    \n",
    "    \n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_NLI_dataset(TRAIN_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(word,vocab):\n",
    "    \"\"\"Accepts word string and vocab, returns id of word in vocab\"\"\"\n",
    "    return vocab.lookup(tf.constant(word)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crate Dataset for NMT Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_NMT_dataset(file1,file2,buffer_size=1024,prefetch_size=5,batch_size=32):\n",
    "    \"\"\"Loads and Processes NMT Dataset\"\"\"\n",
    "    \n",
    "    dataset1 = tf.data.TextLineDataset(file1)\n",
    "    dataset2  = tf.data.TextLineDataset(file2)\n",
    "    \n",
    "    global vocab\n",
    "    global vocab_german\n",
    "    d1 = convertToTokens(dataset1,vocab)\n",
    "    d2 = convertToTokens(dataset2,vocab_german)\n",
    "    \n",
    "    \n",
    "    dataset_nmt = tf.data.Dataset.zip((d1,d2))\n",
    "    dataset_nmt = (dataset_nmt\n",
    "                   .shuffle(buffer_size=buffer_size)\n",
    "                   .padded_batch(batch_size=batch_size,padded_shapes = ({'sentence':[None],'len':[]},\n",
    "                                                                   {'sentence':[None],'len':[]}))\n",
    "                   .prefetch(prefetch_size)\n",
    "                  )\n",
    "    \n",
    "    \n",
    "    return dataset_nmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_nmt  = load_NMT_dataset(EN_FILE,DE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view_data(dataset_nmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tree_dataset(file1,file2,buffer_size=1024,prefetch_size=5,batch_size=32):\n",
    "    \n",
    "    \"\"\"Loads and Processes Tree Dataset\"\"\"\n",
    "    dataset1 = tf.data.TextLineDataset(file1)\n",
    "    dataset2  = tf.data.TextLineDataset(file2)\n",
    "    \n",
    "    global vocab\n",
    "    global vocab_tree\n",
    "    d1 = convertToTokens(dataset1,vocab)\n",
    "    d2 = convertToTokens(dataset2,vocab_tree)\n",
    "    \n",
    "    \n",
    "    dataset_tree = tf.data.Dataset.zip((d1,d2))\n",
    "    dataset_tree = (dataset_tree\n",
    "                   .shuffle(buffer_size=buffer_size)\n",
    "                   .padded_batch(batch_size=batch_size,padded_shapes = ({'sentence':[None],'len':[]},\n",
    "                                                                   {'sentence':[None],'len':[]}))\n",
    "                   .prefetch(prefetch_size)\n",
    "                  )\n",
    "    \n",
    "    \n",
    "    \n",
    "    return dataset_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_tree = load_tree_dataset(CPT_X_FILE,CPT_Y_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view_data(dataset_nli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
