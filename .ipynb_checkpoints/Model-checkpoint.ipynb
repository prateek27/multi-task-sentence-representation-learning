{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(27)\n",
    "DEBUG_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = \"./gensen/data/corpora/allnli.train.txt.clean.noblank\"\n",
    "EN_FILE = \"./english.tok\"\n",
    "DE_FILE = \"./german.tok\"\n",
    "\n",
    "#Const. Parsing FILES\n",
    "CPT_X_FILE = \"./tree_data/en.txt.tok.out\"\n",
    "CPT_Y_FILE  = \"./tree_data/pt.out\"\n",
    "\n",
    "#Vocab Files\n",
    "COMMON_VOCAB_FILE = \"./words.txt\"\n",
    "GERMAN_VOCAB_FILE = \"./german_words.txt\"\n",
    "TREE_VOCAB_FILE = \"./tree_words.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Pipeline'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-bb2df3ac8ba6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Pipeline'"
     ]
    }
   ],
   "source": [
    "import Pipeline as Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "logging = tf.logging\n",
    "logging.set_verbosity(logging.INFO)\n",
    "\n",
    "def log_msg(msg):\n",
    "    logging.info( '{}: {}'.format(time.ctime(),msg ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,V):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.cell = tf.nn.rnn_cell.GRUCell(num_units=512)\n",
    "        self.dense_layer = tf.keras.layers.Dense(V, activation=None)\n",
    "        \n",
    "    def frwrd(self, context_vector,max_len):\n",
    "        \n",
    "        state = self.cell.zero_state(batch_size=int(context_vector.shape[0]), dtype=tf.float32)\n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(max_len): \n",
    "            output, state = self.cell(context_vector,state)\n",
    "            predictions.append(output)\n",
    "            \n",
    "        outputs = tf.stack(predictions,axis=1)\n",
    "        outputs = self.dense_layer(outputs)\n",
    "        \n",
    "        return outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self,V):\n",
    "        super(RNN, self).__init__()\n",
    "        self.EMBED_DIM = 256\n",
    "        self.W = tfe.Variable(tf.random_uniform(minval=-1.0, maxval=1.0, shape=[V, self.EMBED_DIM]))\n",
    "        self.cell = tf.nn.rnn_cell.GRUCell(num_units=512)\n",
    "        #self.cell = tf.contrib.cudnn_rnn.CudnnGRU(num_layers=1, num_units=512)\n",
    "        \n",
    "    def frwrd(self, X, num_words,train=True,dropout=0.3):\n",
    "        u = tf.nn.embedding_lookup(self.W, X)\n",
    "        u_time = tf.unstack(u, axis=1)\n",
    "        #define layer\n",
    "        outputs, final_state = tf.nn.static_rnn(cell=self.cell, inputs=u_time, sequence_length=num_words, dtype=tf.float32)\n",
    "        \n",
    "        if train:\n",
    "            final_state = tf.nn.dropout(final_state, keep_prob=1-dropout)\n",
    "            \n",
    "        return final_state #tf.stack(outputs,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.NUM_CLASSES = 4\n",
    "        self.hidden_layer = tf.keras.layers.Dense(256, activation=tf.nn.relu)\n",
    "        self.output_layer = tf.keras.layers.Dense(4, activation=None)\n",
    "        \n",
    "    def frwrd(self,X):\n",
    "        hidden_output = self.hidden_layer(X)\n",
    "        output = self.output_layer(hidden_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, V,VG ,VT):\n",
    "        \"\"\"\n",
    "        Accepts vocab size \n",
    "        V - English Vocab Size (30002) \n",
    "        VG - German Vocab Size (30002)\n",
    "        VT - Tree Vocab Size (76)\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.rnn = RNN(V)\n",
    "        self.MLP = MLP()\n",
    "        self.decoder = Decoder(VG)\n",
    "        self.tree_decoder = Decoder(VT)\n",
    "        \n",
    "    def frwrd_pass_nli(self, X_Hyp,L_H, X_prem,L_P,train,dropout):\n",
    "        u = self.rnn.frwrd(X_Hyp,L_H,train,dropout)\n",
    "        v = self.rnn.frwrd(X_prem,L_P,train,dropout)\n",
    "        inp = tf.concat( [u,v,tf.abs(u-v), u*v], axis = 1 )\n",
    "        output = self.MLP.frwrd(inp)\n",
    "        return output\n",
    "    \n",
    "    def frwrd_pass_nmt(self, X_En,L_En,maxLen,train,dropout):\n",
    "        context_vector = self.rnn.frwrd(X_En,L_En,train,dropout) #final state from RNN\n",
    "        dec = self.decoder.frwrd(context_vector,maxLen) #maxLen of the the batch going to decoder, from German Dataset\n",
    "        return dec\n",
    "    \n",
    "    def frwrd_pass_cpt(self, X_En,L_En,maxLen,train,dropout):\n",
    "        context_vector = self.rnn.frwrd(X_En,L_En,train,dropout) #final state from RNN\n",
    "        dec = self.tree_decoder.frwrd(context_vector,maxLen) #maxLen of the the batch going to decoder, from German Dataset\n",
    "        return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(30002,30002,76) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_nli(predicted_y, desired_y):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predicted_y, labels=desired_y) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_nmt(predicted_y,desired_y,desired_y_len,max_len):\n",
    "    desired_y = tf.one_hot(desired_y,depth= predicted_y.shape[-1])\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=predicted_y,labels=desired_y)\n",
    "    mask = tf.sequence_mask(desired_y_len,max_len, dtype=tf.float32)\n",
    "    loss = loss * mask\n",
    "    loss = tf.reduce_sum(loss) / tf.cast(tf.reduce_sum(desired_y_len), dtype=tf.float32)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nli(epochs,restore=False,dropout=0.3):\n",
    "    \n",
    "    #Load the dataset\n",
    "    dataset  = Pipeline.load_NLI_dataset(TRAIN_FILE)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = 0.002)\n",
    "    checkpoint_directory = 'models_checkpoints/common/'\n",
    "    checkpoint = tfe.Checkpoint(optimizer=optimizer,\n",
    "                            model=model,\n",
    "                            optimizer_step=tf.train.get_or_create_global_step())\n",
    "    \n",
    "     #Restore latest checkpoint \n",
    "    if(restore):\n",
    "        checkpoint.restore(tf.train.latest_checkpoint(checkpoint_directory))\n",
    "        return\n",
    "\n",
    "        \n",
    "    for i in range(epochs):\n",
    "        cnt = 0\n",
    "        batch_mean_loss = 0.0\n",
    "        for x in iter(dataset):\n",
    "            cnt += 1\n",
    "        \n",
    "            with tf.GradientTape() as tape:\n",
    "                predicted = model.frwrd_pass_nli(x[0]['sentence'],x[0]['len'],x[1]['sentence'],x[1]['len'],True,dropout)\n",
    "                desired = x[2]\n",
    "                curr_loss = loss_nli(predicted, desired)\n",
    "                batch_mean_loss += curr_loss\n",
    "            \n",
    "            grads = tape.gradient( curr_loss, model.variables[:-1] )\n",
    "            optimizer.apply_gradients(zip(grads, model.variables[:-1]),\n",
    "                                        global_step=tf.train.get_or_create_global_step())\n",
    "            \n",
    "            \n",
    "            if(cnt%100==0):\n",
    "                log_msg('Epoch {:d}: Batch Id {:d} NLI Batch Loss: {:.4f}'.format(i,cnt,batch_mean_loss/100.0))\n",
    "                batch_mean_loss = 0.0\n",
    "            \n",
    "            if(DEBUG_MODE):\n",
    "                if(cnt==400):\n",
    "                    break\n",
    "                    \n",
    "        log_msg(\"NLI Training Completed for 1 epoch, Saving Final Checkpoint\")            \n",
    "        checkpoint.save(file_prefix=checkpoint_directory)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nmt(epochs,restore=False,dropout=0.3):\n",
    "    \n",
    "    #Load the Dataset\n",
    "    dataset  = Pipeline.load_NMT_dataset(EN_FILE,DE_FILE,batch_size=8)\n",
    "    \n",
    "    \n",
    "    #Create Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = 0.002)\n",
    "    \n",
    "    \n",
    "    #Init Check Points Directory\n",
    "    checkpoint_directory = 'models_checkpoints/common/'\n",
    "    checkpoint = tfe.Checkpoint(optimizer=optimizer,\n",
    "                            model=model,\n",
    "                            optimizer_step=tf.train.get_or_create_global_step())\n",
    "    \n",
    "    #Restore latest checkpoint \n",
    "    if(restore):\n",
    "        checkpoint.restore(tf.train.latest_checkpoint(checkpoint_directory))\n",
    "\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        cnt=0\n",
    "        batch_mean_loss  = 0.0\n",
    "        for x in iter(dataset):\n",
    "            \n",
    "            curr_loss = 0.0\n",
    "            cnt += 1\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                max_len_of_batch_german = x[1]['sentence'].shape[1]\n",
    "                \n",
    "                max_len_of_batch_english = x[0]['sentence'].shape[1]\n",
    "                if(max_len_of_batch_english>200):\n",
    "                    continue\n",
    "                #print(max_len_of_batch_english)\n",
    "                \n",
    "                input_sentence_english,input_sentence_english_len  = x[0]['sentence'],x[0]['len']\n",
    "                \n",
    "                \n",
    "                predicted = model.frwrd_pass_nmt(input_sentence_english,input_sentence_english_len,max_len_of_batch_german,True,dropout)\n",
    "                #Convert Labels into One Hot\n",
    "                max_batch_len_german = x[1]['sentence'].shape[1]\n",
    "                german_sentence = x[1]['sentence']#tf.one_hot(x[1]['x'],depth=30002)\n",
    "                german_sentence_len  = x[1]['len']\n",
    "                #print(german)\n",
    "                \n",
    "                \n",
    "                curr_loss = loss_nmt(predicted,german_sentence,german_sentence_len,max_batch_len_german)\n",
    "                batch_mean_loss+= curr_loss\n",
    "                #print(('Epoch %d : Batch Id %d Batch Loss: %.4f'%(i,batch_id,curr_loss)))\n",
    "                if(cnt%100==0):\n",
    "                    log_msg('Epoch {:d}: Batch Id {:d} NMT Batch Loss: {:.4f}'.format(i,cnt,batch_mean_loss/100.0))\n",
    "                    checkpoint.save(file_prefix=checkpoint_directory)\n",
    "                    batch_mean_loss = 0 \n",
    "                    \n",
    "                if(DEBUG_MODE):\n",
    "                    if(cnt==400):\n",
    "                        break\n",
    "                        \n",
    "                    \n",
    "        \n",
    "            grads = tape.gradient( curr_loss, model.variables )\n",
    "            optimizer.apply_gradients(zip(grads, model.variables),\n",
    "                                        global_step=tf.train.get_or_create_global_step())\n",
    "         \n",
    "    #Save the Final Checkpoint\n",
    "    log_msg(\"NMT Training Completed, Saving Final Checkpoint\")\n",
    "    checkpoint.save(file_prefix=checkpoint_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cpt(epochs,restore=False,dropout=0.3):\n",
    "    \n",
    "    #Load the Dataset\n",
    "    dataset  = Pipeline.load_tree_dataset(CPT_X_FILE,CPT_Y_FILE)\n",
    "    \n",
    "    \n",
    "    #Create Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = 0.002)\n",
    "    \n",
    "    \n",
    "    #Init Check Points Directory\n",
    "    checkpoint_directory = 'models_checkpoints/common/'\n",
    "    checkpoint = tfe.Checkpoint(optimizer=optimizer,\n",
    "                            model=model,\n",
    "                            optimizer_step=tf.train.get_or_create_global_step())\n",
    "    \n",
    "    #Restore latest checkpoint \n",
    "    if(restore):\n",
    "        checkpoint.restore(tf.train.latest_checkpoint(checkpoint_directory))\n",
    "        log_msg(\"Restored CPT Previous Checkpoint\")\n",
    "        \n",
    "\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        cnt=0\n",
    "        batch_mean_loss  = 0.0\n",
    "        for x in iter(dataset):\n",
    "            curr_loss = 0.0\n",
    "            cnt += 1\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                max_len_of_batch = x[1]['sentence'].shape[1]\n",
    "                input_sentence_english,input_sentence_english_len  = x[0]['sentence'],x[0]['len']\n",
    "                \n",
    "               \n",
    "                predicted = model.frwrd_pass_cpt(input_sentence_english,input_sentence_english_len,max_len_of_batch,True,dropout)\n",
    "                \n",
    "                tree_sentence = x[1]['sentence']#tf.one_hot(x[1]['x'],depth=30002)\n",
    "                tree_sentence_len  = x[1]['len']\n",
    "                \n",
    "                \n",
    "                #Use the same LOSS FUNCTION as NMT\n",
    "                curr_loss = loss_nmt(predicted,tree_sentence,tree_sentence_len,max_len_of_batch)\n",
    "                batch_mean_loss+= curr_loss\n",
    "               \n",
    "                if(cnt%100==0):\n",
    "                    log_msg('Epoch {:d}: Batch Id {:d} CPT Batch Loss: {:.4f}'.format(i,cnt,batch_mean_loss/100.0))\n",
    "                    checkpoint.save(file_prefix=checkpoint_directory)\n",
    "                    batch_mean_loss = 0.0\n",
    "                \n",
    "                if(DEBUG_MODE):\n",
    "                    if(cnt==200):\n",
    "                        break\n",
    "        \n",
    "            grads = tape.gradient(curr_loss, model.variables )\n",
    "            optimizer.apply_gradients(zip(grads, model.variables),\n",
    "                                        global_step=tf.train.get_or_create_global_step())\n",
    "         \n",
    "    #Save the Final Checkpoint\n",
    "    log_msg(\"CPT Training Completed for 1 epoch, Saving Final Checkpoint\")\n",
    "    checkpoint.save(file_prefix=checkpoint_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, model):\n",
    "    if model == 'nli':\n",
    "        train_nli(epochs,False,0.3)\n",
    "    elif model == 'nmt':\n",
    "        train_nmt(epochs,False,0.3)\n",
    "    else:\n",
    "        train_cpt(epochs,False,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Thu Oct 11 16:28:31 2018: Training Started on CPT\n",
      "INFO:tensorflow:Thu Oct 11 16:28:31 2018: Training Completed on CPT\n"
     ]
    }
   ],
   "source": [
    "log_msg(\"Training Started on CPT\")\n",
    "train(1,\"cpt\")\n",
    "log_msg(\"Training Completed on CPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model.rnn.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmy_model = Model(30002,30002,76) \\noptimizer2 = tf.train.AdamOptimizer(learning_rate = 0.002)\\n\\nchkpnt = tfe.Checkpoint(optimizer=optimizer2, model=my_model, optimizer_step=tf.train.get_or_create_global_step())\\ncheckpoint_directory = \\'models_checkpoints/common/\\'\\nprint(chkpnt.restore(tf.train.latest_checkpoint(checkpoint_directory)))\\nlog_msg(\"Restored CPT Previous Checkpoint\")\\nprint(my_model.rnn.W)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "my_model = Model(30002,30002,76) \n",
    "optimizer2 = tf.train.AdamOptimizer(learning_rate = 0.002)\n",
    "\n",
    "chkpnt = tfe.Checkpoint(optimizer=optimizer2, model=my_model, optimizer_step=tf.train.get_or_create_global_step())\n",
    "checkpoint_directory = 'models_checkpoints/common/'\n",
    "print(chkpnt.restore(tf.train.latest_checkpoint(checkpoint_directory)))\n",
    "log_msg(\"Restored CPT Previous Checkpoint\")\n",
    "print(my_model.rnn.W)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Thu Oct 11 16:28:32 2018: Training Started on NLI\n",
      "INFO:tensorflow:Thu Oct 11 16:28:32 2018: Training Completed on NLI\n"
     ]
    }
   ],
   "source": [
    "\n",
    "log_msg(\"Training Started on NLI\")\n",
    "train(1,\"nli\")\n",
    "log_msg(\"Training Completed on NLI\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Thu Oct 11 16:28:32 2018: Training Started on NMT\n",
      "WARNING:tensorflow:From <ipython-input-13-1cd1c71d9d79>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "INFO:tensorflow:Thu Oct 11 16:29:18 2018: Epoch 0: Batch Id 100 NMT Batch Loss: 7.2978\n",
      "INFO:tensorflow:Thu Oct 11 16:30:06 2018: Epoch 0: Batch Id 200 NMT Batch Loss: 6.9562\n",
      "INFO:tensorflow:Thu Oct 11 16:30:50 2018: Epoch 0: Batch Id 300 NMT Batch Loss: 6.9820\n",
      "INFO:tensorflow:Thu Oct 11 16:31:30 2018: Epoch 0: Batch Id 400 NMT Batch Loss: 6.9277\n",
      "INFO:tensorflow:Thu Oct 11 16:31:31 2018: NMT Training Completed, Saving Final Checkpoint\n",
      "INFO:tensorflow:Thu Oct 11 16:31:35 2018: Training Started on NMT\n"
     ]
    }
   ],
   "source": [
    "log_msg(\"Training Started on NMT\")\n",
    "train(1,'nmt')\n",
    "log_msg(\"Training Completed on NMT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 11 16:31:35 2018       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           On   | 0000EA82:00:00.0 Off |                    0 |\r\n",
      "| N/A   54C    P0    56W / 149W |  10898MiB / 11441MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0     31117      C   /anaconda/envs/py36/bin/python             10885MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
